{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6965011e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Loading egg at /opt/anaconda3/lib/python3.11/site-packages/dlib-19.24.99-py3.11-macosx-11.1-arm64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: torchvision in /opt/anaconda3/lib/python3.11/site-packages (0.22.0)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.11/site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: torch==2.7.0 in /opt/anaconda3/lib/python3.11/site-packages (from torchvision) (2.7.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/anaconda3/lib/python3.11/site-packages (from torchvision) (10.2.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.11/site-packages (from torch==2.7.0->torchvision) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/anaconda3/lib/python3.11/site-packages (from torch==2.7.0->torchvision) (4.13.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/anaconda3/lib/python3.11/site-packages (from torch==2.7.0->torchvision) (1.14.0)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.11/site-packages (from torch==2.7.0->torchvision) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.11/site-packages (from torch==2.7.0->torchvision) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.11/site-packages (from torch==2.7.0->torchvision) (2023.10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.11/site-packages (from sympy>=1.13.3->torch==2.7.0->torchvision) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.11/site-packages (from jinja2->torch==2.7.0->torchvision) (2.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1edd9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b807ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def compile_unique_user_csvs(folder_path, output_filename='compiled_output.csv'):\n",
    "    \"\"\"\n",
    "    Combine all CSV files in a folder into one DataFrame, removing duplicate usernames.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): Path to the folder containing CSV files.\n",
    "        output_filename (str): Name of the output CSV file to save in the same folder.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Combined and deduplicated DataFrame.\n",
    "    \"\"\"\n",
    "    all_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "    all_dfs = []\n",
    "\n",
    "    for file in sorted(all_files):\n",
    "        full_path = os.path.join(folder_path, file)\n",
    "        try:\n",
    "            df = pd.read_csv(full_path)\n",
    "            df['source_file'] = file  # Optional: track file origin\n",
    "            all_dfs.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to read {file}: {e}\")\n",
    "\n",
    "    if not all_dfs:\n",
    "        print(\"No CSV files found or all failed to load.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    combined_df = pd.concat(all_dfs, ignore_index=True)\n",
    "\n",
    "    if 'username' in combined_df.columns:\n",
    "        before = len(combined_df)\n",
    "        combined_df.drop_duplicates(subset='username', keep='first', inplace=True)\n",
    "        after = len(combined_df)\n",
    "        print(f\"Removed {before - after} duplicate usernames.\")\n",
    "    else:\n",
    "        print(\"Warning: 'username' column not found; no deduplication applied.\")\n",
    "\n",
    "    output_path = os.path.join(folder_path, output_filename)\n",
    "    combined_df.to_csv(output_path, index=False)\n",
    "    print(f\"Combined CSV saved to: {output_path}\")\n",
    "\n",
    "    return combined_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67fe4dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_unknown_gender_with_avatar(df):\n",
    "    \"\"\"\n",
    "    Filter DataFrame for rows where gender is 'unknown' and avatar is not null.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame with 'gender' and 'avatar' columns.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Filtered DataFrame.\n",
    "    \"\"\"\n",
    "    filtered_df = df[(df['gender'] == 'unknown') & (df['avatar'].notnull())]\n",
    "    print(f\"Filtered down to {len(filtered_df)} rows with unknown gender and valid avatar.\")\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "378be6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "# -------- 1. Dataset class --------\n",
    "class GenderImageDataset(Dataset):\n",
    "    def __init__(self, df, transform=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.df['gender_encoded'] = self.label_encoder.fit_transform(df['new_gender'])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        url = self.df.loc[idx, 'avatar']\n",
    "        label = self.df.loc[idx, 'gender_encoded']\n",
    "        try:\n",
    "            response = requests.get(url, timeout=10)\n",
    "            image = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            return image, torch.tensor(label)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image from {url}: {e}\")\n",
    "            return torch.zeros(3, 224, 224), torch.tensor(label)  # return black image if fail\n",
    "\n",
    "# -------- 2. Training pipeline --------\n",
    "def train_gender_classifier(df, batch_size=16, epochs=5, lr=1e-4):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    train_df, val_df = train_test_split(df, test_size=0.2, stratify=df['new_gender'], random_state=42)\n",
    "    train_dataset = GenderImageDataset(train_df, transform)\n",
    "    val_dataset = GenderImageDataset(val_df, transform)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "    # Use pretrained model\n",
    "    model = models.resnet18(pretrained=True)\n",
    "    model.fc = nn.Linear(model.fc.in_features, 2)  # Binary classification\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}'):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1} loss: {running_loss / len(train_loader):.4f}\")\n",
    "\n",
    "    print(\"Training complete.\")\n",
    "    return model, train_dataset.label_encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bee7470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to read user_attributes_1005.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1006.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1007.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1014.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1015.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1016.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1017.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1018.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1019.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1020.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1021.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1022.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1023.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1024.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1025.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1026.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1027.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1028.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1029.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1030.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1031.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1032.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1033.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1034.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1035.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1036.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1037.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1038.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1039.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1040.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1041.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1042.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1043.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1044.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1045.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1046.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1047.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1048.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1049.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1050.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1051.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1052.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1053.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1054.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1055.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1056.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1057.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1058.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1059.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1060.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1061.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1062.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1063.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1064.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1065.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1066.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1067.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1068.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1069.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1070.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1071.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1072.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1073.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1074.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1075.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1076.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1077.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1078.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1079.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1080.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1081.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1082.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1083.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1084.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1085.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1086.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1087.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1088.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1089.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1090.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1091.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1092.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1093.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1094.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1095.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1096.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1097.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1098.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1099.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1100.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1101.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1102.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1103.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1104.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1105.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1106.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1107.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1108.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1109.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1110.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1111.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1112.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1113.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1114.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1115.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1116.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1117.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1118.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1119.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1120.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1121.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1122.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1123.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1124.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1125.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1126.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1127.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1128.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1129.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1130.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1131.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1132.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1133.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1134.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1135.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1136.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1137.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1138.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1139.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1140.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1141.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1142.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1143.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1144.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1145.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1146.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1147.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1148.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1149.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1150.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1151.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1152.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1153.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1154.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1155.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1156.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1157.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1158.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1159.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1160.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1161.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1162.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1163.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1164.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1165.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1166.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1167.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1168.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1169.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1170.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1171.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1172.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1173.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1174.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1175.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1176.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1177.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1178.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1179.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1180.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1181.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1182.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1183.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1184.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1185.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1186.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1187.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1188.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1189.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1190.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1191.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1192.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1193.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1194.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1195.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1196.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1197.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1198.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1199.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1200.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1201.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1202.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1203.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1204.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1205.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1206.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1207.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1208.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1209.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1210.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1211.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1212.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1213.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1214.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1215.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1216.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1217.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1218.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1219.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1220.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1221.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1222.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1223.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1224.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1225.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1226.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1227.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1228.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1229.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1230.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1231.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1232.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1233.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1234.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1235.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1236.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1237.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1238.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1239.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1240.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1241.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1242.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1243.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1244.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1245.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1246.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1247.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1248.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1249.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1250.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1251.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1252.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1253.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1254.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1255.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1256.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1257.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1258.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1259.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1260.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1261.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1262.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1263.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1264.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1265.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1266.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1267.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1268.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1269.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1270.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1271.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1272.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1273.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1274.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1275.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1276.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1277.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1278.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1279.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1280.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1281.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1282.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1283.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1284.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1285.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1286.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1287.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1288.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1289.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1290.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1291.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1292.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1293.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1294.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1295.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1296.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1297.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1298.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1299.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1300.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1301.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1302.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1303.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1304.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1305.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1306.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1307.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1308.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1309.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1310.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1311.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1312.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1313.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1314.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1315.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1316.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1317.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1318.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1319.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1320.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1321.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1322.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1323.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1324.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1325.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1326.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1327.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1328.csv: No columns to parse from file\n",
      "Failed to read user_attributes_1329.csv: No columns to parse from file\n",
      "Removed 42613 duplicate usernames.\n",
      "Combined CSV saved to: ../outputs/../outputs/compiled_output.csv\n",
      "Filtered down to 20925 rows with unknown gender and valid avatar.\n"
     ]
    }
   ],
   "source": [
    "folder_path = r'../outputs'\n",
    "output_filename = r'../outputs/compiled_output.csv'\n",
    "df = compile_unique_user_csvs(folder_path, output_filename)\n",
    "filtered_df = filter_unknown_gender_with_avatar(df)\n",
    "filtered_df.to_csv(r'../outputs/filtered_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4a28a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_labeled_original = pd.read_csv(r'../outputs/filtered_df_labeled_original.csv')\n",
    "filtered_df_labeled_original_1 = filtered_df_labeled_original.dropna(subset=['new_gender'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "533849e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /Users/chestergarettcalingacion/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 44.7M/44.7M [00:14<00:00, 3.24MB/s]\n",
      "Epoch 1/5: 100%|██████████| 5/5 [03:06<00:00, 37.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 loss: 0.8583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|██████████| 5/5 [00:46<00:00,  9.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 loss: 0.4172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|██████████| 5/5 [00:45<00:00,  9.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 loss: 0.2411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|██████████| 5/5 [00:49<00:00,  9.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 loss: 0.1286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|██████████| 5/5 [01:43<00:00, 20.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 loss: 0.1103\n",
      "Training complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model, label_encoder = train_gender_classifier(filtered_df_labeled_original_1, batch_size=16, epochs=5, lr=1e-4)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f5fa929a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../pickles/label_encoder.pkl']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "import torch\n",
    "\n",
    "# Save PyTorch model\n",
    "torch.save(model.state_dict(), r'../pickles/gender_classifier.pth')\n",
    "\n",
    "# Save LabelEncoder\n",
    "joblib.dump(label_encoder, r'../pickles/label_encoder.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5852b310",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import joblib\n",
    "\n",
    "# Define the same model architecture used during training\n",
    "class GenderClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(GenderClassifier, self).__init__()\n",
    "        self.base = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "        in_features = self.base.fc.in_features\n",
    "        self.base.fc = nn.Linear(in_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.base(x)\n",
    "\n",
    "# Load LabelEncoder\n",
    "le = joblib.load(r'../pickles/label_encoder.pkl')\n",
    "# Load ResNet18 model directly\n",
    "model = models.resnet18()\n",
    "in_features = model.fc.in_features\n",
    "model.fc = nn.Linear(in_features, len(le.classes_))  # Adjust output layer\n",
    "model.load_state_dict(torch.load(r'../pickles/gender_classifier.pth', map_location=torch.device('cpu')))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "51fbcc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_gender_from_url(url, model, label_encoder):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        image = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "        image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(image_tensor)\n",
    "            probs = torch.nn.functional.softmax(output, dim=1)\n",
    "            pred_idx = torch.argmax(probs, dim=1).item()\n",
    "            pred_label = label_encoder.inverse_transform([pred_idx])[0]\n",
    "            confidence = probs[0][pred_idx].item()\n",
    "\n",
    "        return {\n",
    "            'predicted_gender': pred_label,\n",
    "            'confidence': round(confidence, 4)\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'error': str(e),\n",
    "            'predicted_gender': 'unknown',\n",
    "            'confidence': 0.0\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fbb95ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/PIL/Image.py:981: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "filtered_df_labeled_original_2 = filtered_df_labeled_original\n",
    "\n",
    "for idx, row in filtered_df_labeled_original_2.iterrows():\n",
    "    url = row['avatar']  # Replace with the correct column name if different\n",
    "    result = predict_gender_from_url(url, model, label_encoder)\n",
    "    filtered_df_labeled_original_2.loc[idx, 'predicted_gender'] = result['predicted_gender']\n",
    "    filtered_df_labeled_original_2.loc[idx, 'confidence'] = result['confidence']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "45892fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_labeled_original_2.to_csv(r'../outputs/gender_predictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a399df6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e211d47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
